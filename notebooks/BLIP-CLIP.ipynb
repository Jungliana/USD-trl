{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "from trl.core import respond_to_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = load_dataset(\"Multimodal-Fatima/COCO_captions_train\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aleks\\Documents\\Moje dokumenty\\Studia\\Semestr 9\\USD\\USD-trl\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a picture of two kids eating pizza\n"
     ]
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n",
    "\n",
    "url = \"http://farm3.staticflickr.com/2519/4377463269_6c0e733b1b_z.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = \"A picture of\"\n",
    "\n",
    "inputs = processor(image, text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = blip_model.generate(**inputs)\n",
    "out1 = processor.decode(out[0], skip_special_tokens=True)\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two little girls sitting at a table eating pizza\n"
     ]
    }
   ],
   "source": [
    "# unconditional image captioning\n",
    "inputs = processor(image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = blip_model.generate(**inputs)\n",
    "out2 = processor.decode(out[0], skip_special_tokens=True)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption 1: a picture of two kids eating pizza, caption 2: two little girls sitting at a table eating pizza\n",
      "tensor([[32.6343, 34.2727]], grad_fn=<TBackward0>)\n",
      "tensor([[0.1627, 0.8373]], grad_fn=<SoftmaxBackward0>)\n",
      "Caption from clip model: two little girls sitting at a table eating pizza, from dataset: \"two little girls eating a piece of pizza at a table\"\n",
      "tensor([[34.2727, 35.4178]], grad_fn=<TBackward0>)\n",
      "tensor([[0.2414, 0.7586]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "inputs = processor(text=[out1, out2], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(f'Caption 1: {out1}, caption 2: {out2}')\n",
    "outputs = clip_model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "print(logits_per_image)\n",
    "print(probs)\n",
    "\n",
    "inputs = processor(text=[out2, \"two little girls eating a piece of pizza at a table\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(f'Caption from clip model: {out2}, from dataset: \"two little girls eating a piece of pizza at a table\"')\n",
    "outputs = clip_model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "print(logits_per_image)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption 1: two little girls sitting at a table eating pizza\n",
      "tensor([[34.2727]], grad_fn=<TBackward0>)\n",
      "tensor([[1.]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(text=[out2], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(f'Caption 1: {out2}')\n",
    "outputs = clip_model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "print(logits_per_image)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.blip.configuration_blip.BlipConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m processor \u001b[38;5;241m=\u001b[39m BlipProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/blip-image-captioning-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[1;32m----> 4\u001b[0m blip_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLMWithValueHead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalesforce/blip-image-captioning-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m model_ref \u001b[38;5;241m=\u001b[39m create_reference_model(blip_model)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# model = AutoModelForCausalLMWithValueHead.from_pretrained(\"Salesforce/blip-image-captioning-base\")\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# model_ref = create_reference_model(model)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/blip-image-captioning-base\")\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# initialize trainer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aleks\\Documents\\Moje dokumenty\\Studia\\Semestr 9\\USD\\USD-trl\\.venv\\lib\\site-packages\\trl\\models\\modeling_base.py:217\u001b[0m, in \u001b[0;36mPreTrainedModelWrapper.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained peft adapter loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 217\u001b[0m     pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers_parent_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    218\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpretrained_kwargs\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;66;03m# Initialize a new peft adapter with the given config\u001b[39;00m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_loaded_in_8bit \u001b[38;5;129;01mor\u001b[39;00m is_loaded_in_4bit:\n",
      "File \u001b[1;32mc:\\Users\\aleks\\Documents\\Moje dokumenty\\Studia\\Semestr 9\\USD\\USD-trl\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:569\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    568\u001b[0m     )\n\u001b[1;32m--> 569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.blip.configuration_blip.BlipConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig."
     ]
    }
   ],
   "source": [
    "# get models\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "tokenizer = processor.tokenizer\n",
    "blip_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n",
    "model_ref = create_reference_model(blip_model)\n",
    "\n",
    "# model = AutoModelForCausalLMWithValueHead.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "# model_ref = create_reference_model(model)\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# initialize trainer\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# encode a query\n",
    "# query_txt = \"This morning I went to the \"\n",
    "# query_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\")\n",
    "url = \"http://farm3.staticflickr.com/2519/4377463269_6c0e733b1b_z.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = \"A picture of\"\n",
    "\n",
    "inputs = processor(image, text, return_tensors=\"pt\").to(\"cuda\")\n",
    "query_tensor = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# get model response\n",
    "response_tensor  = blip_model.generate(**inputs)\n",
    "\n",
    "# create a ppo trainer\n",
    "ppo_trainer = PPOTrainer(ppo_config, blip_model, model_ref, tokenizer)\n",
    "\n",
    "# define a reward for response\n",
    "# (this could be any reward such as human feedback or output from another model)\n",
    "reward = [torch.tensor(1.0)]\n",
    "\n",
    "# train model for one step with ppo\n",
    "train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
